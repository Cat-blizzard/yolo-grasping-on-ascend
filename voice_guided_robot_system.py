#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è§†è§‰å¼•å¯¼æœºæ¢°è‡‚æŠ“å–ç³»ç»Ÿ - ä¸»æ§ç¨‹åº
æ•´åˆ: è¯­éŸ³è¯†åˆ« â†’ LLMè¯­ä¹‰è§£æ â†’ YOLOv8è§†è§‰æ„ŸçŸ¥ â†’ ç›®æ ‡åŒ¹é… â†’ æœºæ¢°è‡‚æ‰§è¡Œ
"""

import os
import sys
import time
import json
import cv2
import numpy as np
from pathlib import Path
from typing import Dict, List, Tuple, Optional
import logging

# ==================== é…ç½®æ—¥å¿— ====================
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)

# å¯¼å…¥æ·±åº¦å­¦ä¹ æ¡†æ¶ - ä¼˜å…ˆNPU,è‡ªåŠ¨é™çº§CPU
try:
    import mindspore as ms
    from mindspore import Tensor
    MINDSPORE_AVAILABLE = True
    logger.info("âœ… MindSporeå·²å®‰è£… (æ”¯æŒNPUæ¨ç†)")
except ImportError:
    MINDSPORE_AVAILABLE = False
    logger.warning("âš ï¸ MindSporeæœªå®‰è£…,å°†ä½¿ç”¨PyTorch (CPUæ¨¡å¼)")

try:
    import torch
    from torchvision import models, transforms
    TORCH_AVAILABLE = True
    logger.info("âœ… PyTorchå·²å®‰è£… (CPU/GPUå¤‡ç”¨)")
except ImportError:
    TORCH_AVAILABLE = False
    logger.warning("âš ï¸ PyTorchæœªå®‰è£…")

# å¯¼å…¥å­æ¨¡å—
sys.path.insert(0, str(Path(__file__).parent / "mindyolo-master" / "demo"))
sys.path.insert(0, str(Path(__file__).parent / "mindyolo-master"))

from mindyolo.models import create_model
from mindyolo.utils.config import parse_args
from mindyolo.utils.metrics import non_max_suppression, scale_coords, xyxy2xywh

# å¯¼å…¥æœ¬åœ°æ¨¡å—
from mindyolo.demo.recognize_voice import asr_recognize
from mindyolo.demo.LLMæ„å›¾è¯†åˆ« import target_objects

# ROS2ç›¸å…³å¯¼å…¥
try:
    import rclpy
    from dofbot_info.srv import Kinemarics
    import Arm_Lib
    ROS2_AVAILABLE = True
    logger.info("âœ… ROS2æ¨¡å—å·²å®‰è£… (æ”¯æŒæœºæ¢°è‡‚æ§åˆ¶)")
except ImportError:
    ROS2_AVAILABLE = False
    logger.warning("âš ï¸ ROS2æ¨¡å—æœªå®‰è£…,æœºæ¢°è‡‚åŠŸèƒ½å°†è¢«ç¦ç”¨")


# ==================== COCOç±»åˆ«åç§° ====================
COCO_NAMES = [
    "person", "bicycle", "car", "motorcycle", "airplane", "bus", "train", "truck", "boat",
    "traffic light", "fire hydrant", "stop sign", "parking meter", "bench", "bird", "cat",
    "dog", "horse", "sheep", "cow", "elephant", "bear", "zebra", "giraffe", "backpack",
    "umbrella", "handbag", "tie", "suitcase", "frisbee", "skis", "snowboard", "sports ball",
    "kite", "baseball bat", "baseball glove", "skateboard", "surfboard", "tennis racket",
    "bottle", "wine glass", "cup", "fork", "knife", "spoon", "bowl", "banana", "apple",
    "sandwich", "orange", "broccoli", "carrot", "hot dog", "pizza", "donut", "cake", "chair",
    "couch", "potted plant", "bed", "dining table", "toilet", "tv", "laptop", "mouse",
    "remote", "keyboard", "cell phone", "microwave", "oven", "toaster", "sink", "refrigerator",
    "book", "clock", "vase", "scissors", "teddy bear", "hair drier", "toothbrush"
]


# ==================== ä¸­è‹±æ–‡ç‰©å“æ˜ å°„è¡¨ ====================
# ç›®æ ‡ç‰©å“: è‹¹æœã€æ©˜å­ã€æ¯å­ã€ç“¶å­
OBJECT_MAPPING = {
    "è‹¹æœ": "apple",
    "æ©˜å­": "orange",
    "æ©™å­": "orange",
    "æ¯å­": "cup",
    "æ°´æ¯": "cup",
    "ç“¶å­": "bottle",
}

# ==================== åˆ†æ‹£ä½ç½®é…ç½® ====================
# å®šä¹‰4ä¸ªåˆ†æ‹£åŒºåŸŸçš„æ”¾ç½®ä½ç½®(å…³èŠ‚è§’åº¦)
SORTING_POSITIONS = {
    "apple": [45, 50, 20, 60, 265],    # è‹¹æœ - ä½ç½®1(å·¦å‰)
    "orange": [27, 75, 0, 50, 265],    # æ©˜å­ - ä½ç½®2(å·¦å)
    "cup": [147, 75, 0, 50, 265],      # æ¯å­ - ä½ç½®3(å³å)
    "bottle": [133, 50, 20, 60, 265]   # ç“¶å­ - ä½ç½®4(å³å‰)
}


# ==================== è§†è§‰æ„ŸçŸ¥æ¨¡å— ====================
class VisionPerception:
    """è§†è§‰æ„ŸçŸ¥æ¨¡å— - ä¼˜å…ˆNPU,è‡ªåŠ¨é™çº§CPU"""
    
    def __init__(self, model_path_mindir: str = None, model_path_pt: str = None, 
                 config_path: str = None, img_size: int = 640, device: str = "auto"):
        """
        åˆå§‹åŒ–è§†è§‰æ„ŸçŸ¥æ¨¡å— - æ™ºèƒ½é€‰æ‹©æœ€ä¼˜æ¨ç†åç«¯
        
        Args:
            model_path_mindir: MindIRæ¨¡å‹è·¯å¾„(.mindir) - ç”¨äºNPU
            model_path_pt: PyTorchæ¨¡å‹è·¯å¾„(.pt) - ç”¨äºCPU/GPU
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
            img_size: æ¨ç†å›¾åƒå°ºå¯¸
            device: è®¾å¤‡ç±»å‹("auto", "npu", "cpu", "cuda")
        """
        self.img_size = img_size
        self.conf_thres = 0.5  # ç½®ä¿¡åº¦é˜ˆå€¼
        self.iou_thres = 0.65   # NMS IoUé˜ˆå€¼
        self.use_mindspore = False
        self.use_torch = False
        self.backend_name = "Unknown"
        
        logger.info("="*60)
        logger.info("ğŸ”„ åˆå§‹åŒ–è§†è§‰æ„ŸçŸ¥æ¨¡å—")
        logger.info("="*60)
        
        # ç­–ç•¥: ä¼˜å…ˆNPU â†’ GPU â†’ CPU
        if device == "auto":
            device = self._auto_select_device()
        
        logger.info(f"ğŸ“ ç›®æ ‡è®¾å¤‡: {device}")
        
        # 1ï¸âƒ£ ä¼˜å…ˆå°è¯•NPU (MindSpore + æ˜‡è…¾310B)
        if device == "npu" and MINDSPORE_AVAILABLE:
            if self._try_load_npu(model_path_mindir, img_size):
                return
        
        # 2ï¸âƒ£ é™çº§: GPU (PyTorch + CUDA)
        if device == "cuda" and TORCH_AVAILABLE:
            if self._try_load_gpu(model_path_pt):
                return
        
        # 3ï¸âƒ£ æœ€åé™çº§: CPU (PyTorch)
        if TORCH_AVAILABLE:
            if self._try_load_cpu(model_path_pt):
                return
        
        # 4ï¸âƒ£ éƒ½å¤±è´¥åˆ™æŠ¥é”™
        raise RuntimeError(
            "âŒ æ— å¯ç”¨æ¨ç†åç«¯!\n"
            "è¯·å®‰è£…ä»¥ä¸‹ä¹‹ä¸€:\n"
            "  - NPU: pip install mindspore (éœ€æ˜‡è…¾é©±åŠ¨)\n"
            "  - CPU/GPU: pip install ultralytics torch"
        )
    
    def _auto_select_device(self) -> str:
        """è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜è®¾å¤‡"""
        # æ£€æµ‹NPU
        if MINDSPORE_AVAILABLE:
            try:
                import subprocess
                result = subprocess.run(['npu-smi', 'info'], 
                                       capture_output=True, timeout=2)
                if result.returncode == 0:
                    logger.info("âœ… æ£€æµ‹åˆ°æ˜‡è…¾NPU")
                    return "npu"
            except:
                pass
        
        # æ£€æµ‹GPU
        if TORCH_AVAILABLE:
            try:
                import torch
                if torch.cuda.is_available():
                    logger.info(f"âœ… æ£€æµ‹åˆ°CUDA GPU: {torch.cuda.get_device_name(0)}")
                    return "cuda"
            except:
                pass
        
        # é»˜è®¤CPU
        logger.info("â„¹ï¸ ä½¿ç”¨CPUæ¨¡å¼")
        return "cpu"
    
    def _try_load_npu(self, model_path: str, img_size: int) -> bool:
        """å°è¯•åŠ è½½NPUæ¨¡å‹"""
        try:
            logger.info("ğŸš€ å°è¯•åŠ è½½NPUæ¨¡å‹...")
            
            if not model_path or not os.path.exists(model_path):
                logger.warning(f"âš ï¸ NPUæ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
                return False
            
            # è®¾ç½®Ascendä¸Šä¸‹æ–‡
            ms.set_context(mode=ms.GRAPH_MODE, device_target="Ascend", device_id=0)
            ms.set_recursion_limit(2000)
            
            # åŠ è½½MindIRæ¨¡å‹
            graph = ms.load_mindir(model_path)
            self.network = ms.nn.GraphCell(graph)
            
            # é¢„çƒ­ç¼–è¯‘
            dummy = Tensor(np.ones((1, 3, img_size, img_size)), ms.float32)
            _ = self.network(dummy)
            
            self.use_mindspore = True
            self.backend_name = "NPU (Ascend 310B)"
            logger.info("âœ… NPUæ¨¡å‹åŠ è½½æˆåŠŸ! (æ˜‡è…¾310B)")
            logger.info(f"   æ¨¡å‹è·¯å¾„: {model_path}")
            logger.info(f"   é¢„æœŸæ¨ç†é€Ÿåº¦: ~30ms/å¸§")
            return True
            
        except Exception as e:
            logger.warning(f"âš ï¸ NPUåŠ è½½å¤±è´¥: {e}")
            logger.info("   å°†å°è¯•é™çº§åˆ°CPUæ¨¡å¼...")
            return False
    
    def _try_load_gpu(self, model_path: str) -> bool:
        """å°è¯•åŠ è½½GPUæ¨¡å‹"""
        try:
            logger.info("ğŸš€ å°è¯•åŠ è½½GPUæ¨¡å‹...")
            from ultralytics import YOLO
            import torch
            
            if not torch.cuda.is_available():
                logger.warning("âš ï¸ CUDAä¸å¯ç”¨")
                return False
            
            # åŠ è½½YOLOv8æ¨¡å‹
            if model_path and os.path.exists(model_path):
                self.model = YOLO(model_path)
            else:
                logger.info("   ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹ (è‡ªåŠ¨ä¸‹è½½yolov8s.pt)")
                self.model = YOLO('yolov8s.pt')
            
            self.model.to('cuda')
            
            self.use_torch = True
            self.backend_name = f"GPU ({torch.cuda.get_device_name(0)})"
            logger.info("âœ… GPUæ¨¡å‹åŠ è½½æˆåŠŸ!")
            logger.info(f"   é¢„æœŸæ¨ç†é€Ÿåº¦: ~20-50ms/å¸§")
            return True
            
        except Exception as e:
            logger.warning(f"âš ï¸ GPUåŠ è½½å¤±è´¥: {e}")
            return False
    
    def _try_load_cpu(self, model_path: str) -> bool:
        """å°è¯•åŠ è½½CPUæ¨¡å‹"""
        try:
            logger.info("ğŸš€ åŠ è½½CPUæ¨¡å‹...")
            from ultralytics import YOLO
            
            # åŠ è½½YOLOv8æ¨¡å‹
            if model_path and os.path.exists(model_path):
                self.model = YOLO(model_path)
            else:
                logger.info("   ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹ (è‡ªåŠ¨ä¸‹è½½yolov8s.pt)")
                self.model = YOLO('yolov8s.pt')
            
            self.model.to('cpu')
            
            self.use_torch = True
            self.backend_name = "CPU"
            logger.info("âœ… CPUæ¨¡å‹åŠ è½½æˆåŠŸ!")
            logger.info("   é¢„æœŸæ¨ç†é€Ÿåº¦: ~100-200ms/å¸§")
            return True
            
        except Exception as e:
            logger.error(f"âŒ CPUåŠ è½½å¤±è´¥: {e}")
            return False
    
    def detect(self, img: np.ndarray) -> Dict:
        """æ‰§è¡Œç›®æ ‡æ£€æµ‹ - è‡ªåŠ¨é€‰æ‹©åç«¯"""
        if self.use_torch:
            return self._detect_torch(img)
        elif self.use_mindspore:
            return self._detect_mindspore(img)
        else:
            raise RuntimeError("æ— å¯ç”¨æ£€æµ‹åç«¯")
    
    def _detect_torch(self, img: np.ndarray) -> Dict:
        """
        PyTorch/Ultralytics YOLOæ£€æµ‹
        
        Args:
            img: BGRæ ¼å¼çš„è¾“å…¥å›¾åƒ
            
        Returns:
            æ£€æµ‹ç»“æœå­—å…¸
        """
        t0 = time.time()
        
        # Ultralytics YOLOæ¨ç†
        results = self.model(img, conf=self.conf_thres, iou=self.iou_thres, verbose=False)
        result = results[0]  # ç¬¬ä¸€å¼ å›¾
        
        infer_time = time.time() - t0
        logger.info(f"â±ï¸ æ¨ç†è€—æ—¶: {infer_time*1000:.1f}ms ({self.backend_name})")
        
        # è§£æç»“æœ
        detections = []
        boxes = result.boxes
        
        for box in boxes:
            x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()
            conf = float(box.conf[0])
            cls_id = int(box.cls[0])
            
            # è®¡ç®—ä¸­å¿ƒç‚¹
            cx = int((x1 + x2) / 2)
            cy = int((y1 + y2) / 2)
            
            detections.append({
                "class_id": cls_id,
                "class_name": COCO_NAMES[cls_id] if cls_id < len(COCO_NAMES) else "unknown",
                "confidence": conf,
                "bbox": [float(x1), float(y1), float(x2), float(y2)],
                "center": (cx, cy)
            })
        
        logger.info(f"ğŸ‘ï¸ æ£€æµ‹åˆ° {len(detections)} ä¸ªç‰©ä½“")
        return {"detections": detections}
    
    def _detect_mindspore(self, img: np.ndarray) -> Dict:
        """
        MindSporeæ£€æµ‹(å¤‡ç”¨)
        
        Args:
            img: BGRæ ¼å¼çš„è¾“å…¥å›¾åƒ
            
        Returns:
            æ£€æµ‹ç»“æœå­—å…¸
        """
        h_ori, w_ori = img.shape[:2]
        # 1. å›¾åƒé¢„å¤„ç†
        r = self.img_size / max(h_ori, w_ori)
        if r != 1:
            interp = cv2.INTER_AREA if r < 1 else cv2.INTER_LINEAR
            img_resized = cv2.resize(img, (int(w_ori * r), int(h_ori * r)), interpolation=interp)
        else:
            img_resized = img.copy()
        
        # Paddingåˆ°640x640
        h, w = img_resized.shape[:2]
        if h < self.img_size or w < self.img_size:
            dh = (self.img_size - h) / 2
            dw = (self.img_size - w) / 2
            img_resized = cv2.copyMakeBorder(
                img_resized, int(dh), int(dh), int(dw), int(dw),
                cv2.BORDER_CONSTANT, value=(114, 114, 114)
            )
        
        # è½¬æ¢ä¸ºTensor (NCHW, RGB, [0,1])
        img_tensor = img_resized[:, :, ::-1].transpose(2, 0, 1) / 255.0
        img_tensor = Tensor(img_tensor[None], ms.float32)
        
        # 2. NPUæ¨ç†
        t0 = time.time()
        out = self.network(img_tensor)
        infer_time = time.time() - t0
        
        # 3. NMSåå¤„ç†
        out = out.asnumpy()
        t1 = time.time()
        out = non_max_suppression(out, conf_thres=self.conf_thres, iou_thres=self.iou_thres, need_nms=True)
        nms_time = time.time() - t1
        
        logger.info(f"â±ï¸ æ¨ç†è€—æ—¶: {infer_time*1000:.1f}ms | NMS: {nms_time*1000:.1f}ms")
        
        # 4. è§£æç»“æœ
        detections = []
        for pred in out:
            if len(pred) == 0:
                continue
            
            # åæ ‡æ˜ å°„å›åŸå›¾
            predn = np.copy(pred)
            scale_coords(img_tensor.shape[2:], predn[:, :4], (h_ori, w_ori))
            
            for det in predn:
                x1, y1, x2, y2, conf, cls_id = det
                cls_id = int(cls_id)
                
                # è®¡ç®—ä¸­å¿ƒç‚¹
                cx = int((x1 + x2) / 2)
                cy = int((y1 + y2) / 2)
                
                detections.append({
                    "class_id": cls_id,
                    "class_name": COCO_NAMES[cls_id] if cls_id < len(COCO_NAMES) else "unknown",
                    "confidence": float(conf),
                    "bbox": [float(x1), float(y1), float(x2), float(y2)],
                    "center": (cx, cy)
                })
        
        logger.info(f"ğŸ‘ï¸ æ£€æµ‹åˆ° {len(detections)} ä¸ªç‰©ä½“")
        return {"detections": detections}


# ==================== å†³ç­–å±‚æ¨¡å— ====================
class DecisionLayer:
    """å†³ç­–å±‚ - è´Ÿè´£ç›®æ ‡åŒ¹é…å’Œä»»åŠ¡ç”Ÿæˆ"""
    
    @staticmethod
    def match_target(voice_target: str, visual_detections: List[Dict]) -> Optional[Dict]:
        """
        åŒ¹é…è¯­éŸ³ç›®æ ‡ä¸è§†è§‰æ£€æµ‹ç»“æœ
        
        Args:
            voice_target: è¯­éŸ³æå–çš„ä¸­æ–‡ç›®æ ‡(å¦‚"æ°´æ¯")
            visual_detections: è§†è§‰æ£€æµ‹ç»“æœåˆ—è¡¨
            
        Returns:
            åŒ¹é…æˆåŠŸè¿”å›æœ€ä½³ç›®æ ‡,å¤±è´¥è¿”å›None
        """
        # 1. ä¸­è‹±æ–‡æ˜ å°„
        english_target = OBJECT_MAPPING.get(voice_target)
        if not english_target:
            logger.warning(f"âš ï¸ æœªæ‰¾åˆ°'{voice_target}'çš„è‹±æ–‡æ˜ å°„")
            return None
        
        logger.info(f"ğŸ” åŒ¹é…ç›®æ ‡: {voice_target} â†’ {english_target}")
        
        # 2. åœ¨æ£€æµ‹ç»“æœä¸­æŸ¥æ‰¾
        candidates = [
            det for det in visual_detections 
            if det["class_name"] == english_target
        ]
        
        if not candidates:
            logger.warning(f"âŒ æœªæ£€æµ‹åˆ°ç›®æ ‡ç‰©ä½“: {english_target}")
            return None
        
        # 3. é€‰æ‹©æœ€é«˜ç½®ä¿¡åº¦çš„ç›®æ ‡
        best_match = max(candidates, key=lambda x: x["confidence"])
        logger.info(f"âœ… åŒ¹é…æˆåŠŸ: {best_match['class_name']} (ç½®ä¿¡åº¦: {best_match['confidence']:.2%})")
        
        return best_match


# ==================== åæ ‡æ˜ å°„æ¨¡å— ====================
class CoordinateMapper:
    """åæ ‡æ˜ å°„æ¨¡å— - åƒç´ åæ ‡è½¬æœºæ¢°è‡‚åŸºåæ ‡"""
    
    def __init__(self, offset_path: str, dp_bin_path: Optional[str] = None):
        """
        åˆå§‹åŒ–åæ ‡æ˜ å°„
        
        Args:
            offset_path: offset.txtæ–‡ä»¶è·¯å¾„
            dp_bin_path: é€è§†å˜æ¢å‚æ•°æ–‡ä»¶è·¯å¾„(å¯é€‰)
        """
        # è¯»å–åç§»é‡
        with open(offset_path, 'r') as f:
            self.y_offset = float(f.readline().strip())
            self.x_offset = float(f.readline().strip())
        
        logger.info(f"ğŸ“ åæ ‡åç§»: x={self.x_offset}, y={self.y_offset}")
        
        self.dp_bin_path = dp_bin_path
    
    def pixel_to_robot_base(self, pixel_x: int, pixel_y: int, img_width: int = 640, img_height: int = 480) -> Tuple[float, float]:
        """
        åƒç´ åæ ‡è½¬æœºæ¢°è‡‚åŸºåæ ‡
        
        Args:
            pixel_x, pixel_y: åƒç´ åæ ‡
            img_width, img_height: å›¾åƒå°ºå¯¸
            
        Returns:
            (x, y) æœºæ¢°è‡‚åŸºåæ ‡ç³»ä¸‹çš„åæ ‡
        """
        # å‚è€ƒgarbage_identify.pyçš„è½¬æ¢å…¬å¼
        a = round(((pixel_x - 320) / 4000), 5)
        b = round(((480 - pixel_y) / 3000) * 0.8 + 0.19, 5)
        
        # åº”ç”¨åç§»è¡¥å¿
        x = a + self.x_offset
        y = b + self.y_offset
        
        logger.info(f"ğŸ“ åæ ‡æ˜ å°„: åƒç´ ({pixel_x}, {pixel_y}) â†’ æœºæ¢°è‡‚({x:.4f}, {y:.4f})")
        return (x, y)


# ==================== æœºæ¢°è‡‚æ‰§è¡Œæ¨¡å— ====================
class RobotArmController:
    """æœºæ¢°è‡‚æ‰§è¡Œæ¨¡å— - ROS2æœåŠ¡è°ƒç”¨ + é€†è¿åŠ¨å­¦"""
    
    def __init__(self):
        """åˆå§‹åŒ–æœºæ¢°è‡‚æ§åˆ¶å™¨"""
        if not ROS2_AVAILABLE:
            logger.error("âŒ ROS2ä¸å¯ç”¨,æœºæ¢°è‡‚æ§åˆ¶å™¨åˆå§‹åŒ–å¤±è´¥")
            return
        
        # æ£€æŸ¥ROS2æ˜¯å¦å·²åˆå§‹åŒ–(å‚è€ƒgarbage_identify.py line 30)
        if not rclpy.ok():
            rclpy.init()
            logger.info("ğŸ”§ ROS2åˆå§‹åŒ–å®Œæˆ")
        
        self.node = rclpy.create_node("voice_robot_controller")
        self.client = self.node.create_client(Kinemarics, "trial_service")
        self.arm = Arm_Lib.Arm_Device()
        
        logger.info("âœ… æœºæ¢°è‡‚æ§åˆ¶å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def inverse_kinematics(self, x: float, y: float, z: float = 0.0) -> List[float]:
        """
        è°ƒç”¨ROS2é€†è¿åŠ¨å­¦æœåŠ¡
        
        Args:
            x, y, z: ç›®æ ‡ä½å§¿(åŸºåæ ‡ç³»)
            
        Returns:
            å…³èŠ‚è§’åº¦åˆ—è¡¨ [j1, j2, j3, j4, j5]
        """
        if not ROS2_AVAILABLE:
            return [90, 90, 0, 0, 90]
        
        self.client.wait_for_service(timeout_sec=2.0)
        
        request = Kinemarics.Request()
        request.tar_x = x
        request.tar_y = y
        request.tar_z = z
        request.kin_name = "ik"
        
        future = self.client.call_async(request)
        rclpy.spin_until_future_complete(self.node, future)
        
        response = future.result()
        if response:
            joints = [
                response.joint1,
                response.joint2,
                response.joint3,
                response.joint4,
                response.joint5
            ]
            
            # è§’åº¦è°ƒæ•´(å‚è€ƒgarbage_identify.py)
            if joints[2] < 0:
                joints[1] += joints[2] / 2
                joints[3] += joints[2] * 3 / 4
                joints[2] = 0
            
            logger.info(f"ğŸ¤– é€†è¿åŠ¨å­¦è§£ç®—: ({x:.3f}, {y:.3f}, {z:.3f}) â†’ {joints}")
            return joints
        
        return None
    
    def grasp_and_place(self, joints: List[float], target_class: str, xy_init: List[int] = [90, 135]):
        """
        æ‰§è¡ŒæŠ“å–+åˆ†æ‹£åŠ¨ä½œ
        
        Args:
            joints: ç›®æ ‡å…³èŠ‚è§’åº¦
            target_class: ç›®æ ‡ç±»åˆ«("apple", "orange", "cup", "bottle")
            xy_init: åˆå§‹ä½ç½®
        """
        if not ROS2_AVAILABLE:
            logger.warning("âš ï¸ ROS2ä¸å¯ç”¨,è·³è¿‡æœºæ¢°è‡‚åŠ¨ä½œ")
            return
        
        logger.info(f"ğŸ¤– å¼€å§‹æ‰§è¡ŒæŠ“å–åŠ¨ä½œ: {target_class}")
        
        # èœ‚é¸£å™¨æç¤º
        self.arm.Arm_Buzzer_On(1)
        time.sleep(0.5)
        
        grap_joint = 130  # å¤¹çˆªé—­åˆè§’åº¦
        
        # 1. ç§»åŠ¨åˆ°ç›®æ ‡ä¸Šæ–¹
        joints_up = [joints[0], 80, 50, 50, 265, 30]
        self.arm.Arm_serial_servo_write6_array(joints_up, 1000)
        time.sleep(1)
        
        # 2. æ¾å¼€å¤¹çˆª
        self.arm.Arm_serial_servo_write(6, 0, 500)
        time.sleep(0.5)
        
        # 3. ç§»åŠ¨åˆ°ç›®æ ‡ä½ç½®
        joints_target = [joints[0], joints[1], joints[2], joints[3], 265, 30]
        self.arm.Arm_serial_servo_write6_array(joints_target, 500)
        time.sleep(0.5)
        
        # 4. å¤¹ç´§
        self.arm.Arm_serial_servo_write(6, grap_joint, 500)
        time.sleep(0.5)
        
        # 5. æŠ¬èµ·
        self.arm.Arm_serial_servo_write6_array(joints_up, 1000)
        time.sleep(1)
        
        # 6. ç§»åŠ¨åˆ°åˆ†æ‹£ä½ç½®
        if target_class in SORTING_POSITIONS:
            sorting_joints = SORTING_POSITIONS[target_class] + [grap_joint]
            logger.info(f"ğŸ“¦ ç§»åŠ¨åˆ°åˆ†æ‹£ä½ç½®: {target_class}")
            self.arm.Arm_serial_servo_write6_array(sorting_joints, 1000)
            time.sleep(1)
            
            # 7. é‡Šæ”¾ç‰©ä½“
            self.arm.Arm_serial_servo_write(6, 30, 500)
            time.sleep(0.5)
        else:
            logger.warning(f"âš ï¸ æœªæ‰¾åˆ°{target_class}çš„åˆ†æ‹£ä½ç½®,æ”¾å›åˆå§‹ä½ç½®")
        
        # 8. è¿”å›åˆå§‹ä½ç½®
        joints_init = [xy_init[0], xy_init[1], 0, 0, 90, 30]
        self.arm.Arm_serial_servo_write6_array(joints_init, 1000)
        time.sleep(1)
        
        logger.info("âœ… æŠ“å–åŠ¨ä½œå®Œæˆ")


# ==================== ä¸»ç³»ç»Ÿ ====================
class VoiceGuidedRobotSystem:
    """è¯­éŸ³å¼•å¯¼æœºæ¢°è‡‚ç³»ç»Ÿ - ä¸»æ§ç±»"""
    
    def __init__(self, config: Dict):
        """
        åˆå§‹åŒ–ç³»ç»Ÿ
        
        Args:
            config: é…ç½®å­—å…¸ {
                "model_path": YOLOv8æ¨¡å‹è·¯å¾„,
                "offset_path": offset.txtè·¯å¾„,
                "camera_id": æ‘„åƒå¤´ID
            }
        """
        logger.info("="*60)
        logger.info("ğŸš€ å¯åŠ¨è§†è§‰å¼•å¯¼æœºæ¢°è‡‚æŠ“å–ç³»ç»Ÿ")
        logger.info("="*60)
        
        # 1. åˆå§‹åŒ–è§†è§‰æ¨¡å— - ä¼˜å…ˆNPU
        model_path_mindir = config.get("model_path_mindir")  # NPUæ¨¡å‹
        model_path_pt = config.get("model_path_pt")  # CPU/GPUæ¨¡å‹
        device = config.get("device", "auto")  # auto/npu/cpu/cuda
        
        self.vision = VisionPerception(
            model_path_mindir=model_path_mindir,
            model_path_pt=model_path_pt,
            config_path=None,
            img_size=640,
            device=device
        )
        
        # 2. åˆå§‹åŒ–åæ ‡æ˜ å°„
        self.mapper = CoordinateMapper(offset_path=config["offset_path"])
        
        # 3. åˆå§‹åŒ–æœºæ¢°è‡‚æ§åˆ¶å™¨
        self.robot = RobotArmController() if ROS2_AVAILABLE else None
        
        # 4. æ‰“å¼€æ‘„åƒå¤´
        self.camera_id = config.get("camera_id", 0)
        self.cap = cv2.VideoCapture(self.camera_id)
        if not self.cap.isOpened():
            raise RuntimeError(f"âŒ æ— æ³•æ‰“å¼€æ‘„åƒå¤´ {self.camera_id}")
        
        self.cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)
        self.cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)
        self.cap.set(cv2.CAP_PROP_FOURCC, cv2.VideoWriter_fourcc('M', 'J', 'P', 'G'))
        
        logger.info("âœ… ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
    
    def run_once(self, enable_voice=True, enable_llm=True):
        """æ‰§è¡Œä¸€æ¬¡å®Œæ•´çš„æŠ“å–æµç¨‹"""
        logger.info("\n" + "="*60)
        logger.info("ğŸ™ï¸ æ­¥éª¤1: è¯­éŸ³è¾“å…¥")
        logger.info("="*60)
        
        voice_text = ""
        target_name = ""
        
        # 1. è¯­éŸ³è¯†åˆ«(å¯é€‰)
        if enable_voice:
            print("\nâ–¶ï¸ è¯·è¯´å‡ºæ‚¨çš„æŒ‡ä»¤(å¦‚: å¸®æˆ‘æ‹¿è‹¹æœ)...")
            try:
                voice_text = asr_recognize(max_duration=5.0, interval_sec=0.04)
                logger.info(f"ğŸ“ è¯†åˆ«ç»“æœ: {voice_text}")
            except Exception as e:
                logger.error(f"âŒ è¯­éŸ³è¯†åˆ«å¤±è´¥: {e}")
                enable_llm = False
        
        # 2. LLMè¯­ä¹‰è§£æ(å¯é€‰)
        if enable_llm and voice_text:
            logger.info("\n" + "="*60)
            logger.info("ğŸ§  æ­¥éª¤2: è¯­ä¹‰è§£æ")
            logger.info("="*60)
            
            try:
                target_list = target_objects(voice_text)
                if not target_list:
                    logger.warning("âš ï¸ æœªè¯†åˆ«åˆ°ç›®æ ‡ç‰©å“")
                    return False
                
                target_name = target_list[0]  # å–ç¬¬ä¸€ä¸ªç›®æ ‡
                logger.info(f"ğŸ¯ æå–ç›®æ ‡: {target_name}")
            except Exception as e:
                logger.error(f"âŒ è¯­ä¹‰è§£æå¤±è´¥: {e}")
                return False
        
        # å¦‚æœæ²¡æœ‰è¯­éŸ³è¾“å…¥,ç›´æ¥æ£€æµ‹æ‰€æœ‰4ä¸ªç±»åˆ«
        if not target_name:
            logger.info("â„¹ï¸ æ— è¯­éŸ³è¾“å…¥,å°†æ£€æµ‹æ‰€æœ‰ç›®æ ‡ç±»åˆ«: è‹¹æœ/æ©˜å­/æ¯å­/ç“¶å­")
        
        # 3. è§†è§‰æ„ŸçŸ¥
        logger.info("\n" + "="*60)
        logger.info("ğŸ‘ï¸ æ­¥éª¤3: è§†è§‰æ„ŸçŸ¥")
        logger.info("="*60)
        
        ret, frame = self.cap.read()
        if not ret:
            logger.error("âŒ æ‘„åƒå¤´è¯»å–å¤±è´¥")
            return False
        
        # è°ƒæ•´å›¾åƒå°ºå¯¸
        frame = cv2.resize(frame, (640, 480))
        
        # æ‰§è¡Œæ£€æµ‹
        result = self.vision.detect(frame)
        detections = result["detections"]
        
        # å¯è§†åŒ–
        vis_img = frame.copy()
        for det in detections:
            x1, y1, x2, y2 = [int(v) for v in det["bbox"]]
            cv2.rectangle(vis_img, (x1, y1), (x2, y2), (0, 255, 0), 2)
            cv2.putText(vis_img, f"{det['class_name']} {det['confidence']:.2f}", 
                       (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)
            cx, cy = det["center"]
            cv2.circle(vis_img, (cx, cy), 5, (0, 0, 255), -1)
        
        cv2.imshow("Detection Result", vis_img)
        cv2.waitKey(2000)  # æ˜¾ç¤º2ç§’
        
        # 4. ç›®æ ‡åŒ¹é…
        logger.info("\n" + "="*60)
        logger.info("ğŸ” æ­¥éª¤4: ç›®æ ‡åŒ¹é…")
        logger.info("="*60)
        
        matched_target = DecisionLayer.match_target(target_name, detections)
        if not matched_target:
            logger.warning(f"âŒ æœªæ‰¾åˆ°ç›®æ ‡ç‰©ä½“: {target_name}")
            return False
        
        # 5. åæ ‡æ˜ å°„
        logger.info("\n" + "="*60)
        logger.info("ğŸ“ æ­¥éª¤5: åæ ‡æ˜ å°„")
        logger.info("="*60)
        
        cx, cy = matched_target["center"]
        robot_x, robot_y = self.mapper.pixel_to_robot_base(cx, cy)
        
        # 6. é€†è¿åŠ¨å­¦+æ‰§è¡Œ
        logger.info("\n" + "="*60)
        logger.info("ğŸ¤– æ­¥éª¤6: æœºæ¢°è‡‚æ‰§è¡Œ")
        logger.info("="*60)
        
        if self.robot:
            joints = self.robot.inverse_kinematics(robot_x, robot_y, z=0.0)
            if joints:
                # ä¼ é€’target_classè¿›è¡Œåˆ†æ‹£
                english_target = OBJECT_MAPPING.get(target_name, matched_target["class_name"])
                self.robot.grasp_and_place(joints, target_class=english_target)
                logger.info("âœ… ä»»åŠ¡å®Œæˆ!")
                return True
        else:
            logger.warning("âš ï¸ æœºæ¢°è‡‚ä¸å¯ç”¨,ä»…æ¨¡æ‹Ÿåæ ‡æ˜ å°„")
            logger.info(f"ğŸ“ ç›®æ ‡åæ ‡: ({robot_x:.4f}, {robot_y:.4f})")
        
        return True
    
    def run_continuous(self):
        """æŒç»­è¿è¡Œæ¨¡å¼"""
        logger.info("\nğŸ” è¿›å…¥æŒç»­è¿è¡Œæ¨¡å¼ (æŒ‰Ctrl+Cé€€å‡º)")
        
        try:
            while True:
                self.run_once()
                logger.info("\nâ³ ç­‰å¾…5ç§’åæ‰§è¡Œä¸‹ä¸€æ¬¡...")
                time.sleep(5)
        except KeyboardInterrupt:
            logger.info("\nğŸ›‘ ç”¨æˆ·ä¸­æ–­,ç³»ç»Ÿé€€å‡º")
        finally:
            self.cleanup()
    
    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        if self.cap:
            self.cap.release()
        cv2.destroyAllWindows()
        logger.info("â™»ï¸ èµ„æºå·²é‡Šæ”¾")


# ==================== ä¸»å…¥å£ ====================
def main():
    # é…ç½®å‚æ•° - ä¼˜å…ˆNPU,è‡ªåŠ¨é™çº§CPU
    import platform
    
    # åŸºç¡€é…ç½®
    config = {
        "model_path_mindir": None,  # NPUæ¨¡å‹(.mindir)
        "model_path_pt": None,      # CPU/GPUæ¨¡å‹(.pt)
        "offset_path": "",
        "camera_id": 0,
        "device": "auto"  # auto: è‡ªåŠ¨é€‰æ‹©(NPU>GPU>CPU)
    }
    
    # æ ¹æ®æ“ä½œç³»ç»Ÿè®¾ç½®è·¯å¾„
    if platform.system() == "Windows":
        # Windowsç¯å¢ƒ - æ”¯æŒNPU/CPU
        config["model_path_mindir"] = r"d:\robocode\mindyolo-master\yolov8s_coco.mindir"  # NPUæ¨¡å‹
        config["model_path_pt"] = r"d:\robocode\mindyolo-master\yolov8s.pt"  # CPUå¤‡ç”¨
        config["offset_path"] = r"d:\robocode\ros2_robot_arm\ros2_ws\src\dofbot_garbage_yolov5\dofbot_garbage_yolov5\config\offset.txt"
    else:  # Linux/Ubuntu
        # Ubuntuç¯å¢ƒ - ä»…CPU
        config["model_path_pt"] = "/home/user/robocode/mindyolo-master/yolov8s.pt"
        config["offset_path"] = "/home/user/robocode/ros2_robot_arm/ros2_ws/src/dofbot_garbage_yolov5/dofbot_garbage_yolov5/config/offset.txt"
    
    logger.info("\n" + "="*70)
    logger.info("ğŸ¯ ç³»ç»Ÿå¯åŠ¨ä¿¡æ¯")
    logger.info("="*70)
    logger.info(f"ğŸ’» æ“ä½œç³»ç»Ÿ: {platform.system()}")
    logger.info(f"ğŸ“¦ NPUæ¨¡å‹: {config['model_path_mindir'] or 'æœªé…ç½®'}")
    logger.info(f"ğŸ“¦ CPUæ¨¡å‹: {config['model_path_pt'] or 'è‡ªåŠ¨ä¸‹è½½'}")
    logger.info(f"ğŸ¯ è®¾å¤‡æ¨¡å¼: {config['device']} (ä¼˜å…ˆNPU)")
    logger.info(f"ğŸ¯ ç›®æ ‡ç‰©å“: è‹¹æœ/æ©˜å­/æ¯å­/ç“¶å­")
    logger.info("="*70 + "\n")
    
    # åˆ›å»ºç³»ç»Ÿå®ä¾‹
    system = VoiceGuidedRobotSystem(config)
    
    # è¿è¡Œæ¨¡å¼é€‰æ‹©
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument("--mode", choices=["once", "continuous", "vision_only"], default="vision_only",
                       help="è¿è¡Œæ¨¡å¼: once(å•æ¬¡) æˆ– continuous(æŒç»­) æˆ– vision_only(ä»…è§†è§‰)")
    parser.add_argument("--no-voice", action="store_true", help="ç¦ç”¨è¯­éŸ³è¯†åˆ«")
    parser.add_argument("--no-llm", action="store_true", help="ç¦ç”¨LLMè§£æ")
    args = parser.parse_args()
    
    if args.mode == "once":
        system.run_once(enable_voice=not args.no_voice, enable_llm=not args.no_llm)
    elif args.mode == "vision_only":
        logger.info("ğŸ‘ï¸ ä»…è§†è§‰æ£€æµ‹æ¨¡å¼ (æ— è¯­éŸ³/LLM)")
        system.run_once(enable_voice=False, enable_llm=False)
    else:
        system.run_continuous()


if __name__ == "__main__":
    main()
