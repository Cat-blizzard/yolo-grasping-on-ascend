#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç®€åŒ–ç‰ˆæµ‹è¯•è„šæœ¬ - ä¸ä¾èµ–æœºæ¢°è‡‚,ä»…æµ‹è¯•è§†è§‰æ£€æµ‹+è¯­éŸ³+LLMåŒ¹é…
"""

import os
import sys
import time
import cv2
import numpy as np
from pathlib import Path
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s'
)
logger = logging.getLogger(__name__)

# æ·»åŠ è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent / "mindyolo-master" / "demo"))
sys.path.insert(0, str(Path(__file__).parent / "mindyolo-master"))


def test_voice_recognition():
    """æµ‹è¯•è¯­éŸ³è¯†åˆ«æ¨¡å—"""
    logger.info("="*60)
    logger.info("æµ‹è¯•1: è¯­éŸ³è¯†åˆ«")
    logger.info("="*60)
    
    try:
        from recognize_voice import asr_recognize
        
        print("\nè¯·è¯´è¯(5ç§’)...")
        text = asr_recognize(max_duration=5.0, interval_sec=0.04)
        logger.info(f"âœ… è¯†åˆ«ç»“æœ: {text}")
        return text
    except Exception as e:
        logger.error(f"âŒ è¯­éŸ³è¯†åˆ«å¤±è´¥: {e}")
        return None


def test_llm_parsing(text):
    """æµ‹è¯•LLMè¯­ä¹‰è§£æ"""
    logger.info("="*60)
    logger.info("æµ‹è¯•2: LLMè¯­ä¹‰è§£æ")
    logger.info("="*60)
    
    try:
        from LLMæ„å›¾è¯†åˆ« import target_objects
        
        targets = target_objects(text)
        logger.info(f"âœ… æå–ç›®æ ‡: {targets}")
        return targets
    except Exception as e:
        logger.error(f"âŒ LLMè§£æå¤±è´¥: {e}")
        return None


def test_vision_detection():
    """æµ‹è¯•è§†è§‰æ£€æµ‹æ¨¡å—"""
    logger.info("="*60)
    logger.info("æµ‹è¯•3: è§†è§‰æ£€æµ‹(ä½¿ç”¨predict_1.py)")
    logger.info("="*60)
    
    try:
        import mindspore as ms
        from mindspore import Tensor
        from mindyolo.utils.metrics import non_max_suppression, scale_coords
        
        # è®¾ç½®ç¯å¢ƒ
        ms.set_context(mode=ms.GRAPH_MODE, device_target="Ascend", device_id=0)
        
        # åŠ è½½æ¨¡å‹
        model_path = r"d:\robocode\mindyolo-master\yolov8s_coco.mindir"
        logger.info(f"åŠ è½½æ¨¡å‹: {model_path}")
        
        graph = ms.load_mindir(model_path)
        network = ms.nn.GraphCell(graph)
        
        # é¢„çƒ­
        dummy = Tensor(np.ones((1, 3, 640, 640)), ms.float32)
        _ = network(dummy)
        logger.info("âœ… æ¨¡å‹é¢„çƒ­å®Œæˆ")
        
        # æ‰“å¼€æ‘„åƒå¤´
        cap = cv2.VideoCapture(0)
        ret, frame = cap.read()
        cap.release()
        
        if not ret:
            logger.error("âŒ æ‘„åƒå¤´è¯»å–å¤±è´¥")
            return None
        
        # é¢„å¤„ç†
        img = cv2.resize(frame, (640, 480))
        h_ori, w_ori = img.shape[:2]
        
        img_input = cv2.resize(img, (640, 640))
        img_tensor = img_input[:, :, ::-1].transpose(2, 0, 1) / 255.0
        img_tensor = Tensor(img_tensor[None], ms.float32)
        
        # æ¨ç†
        t0 = time.time()
        out = network(img_tensor)
        infer_time = time.time() - t0
        
        # NMS
        out = out.asnumpy()
        out = non_max_suppression(out, conf_thres=0.5, iou_thres=0.65, need_nms=True)
        
        logger.info(f"â±ï¸ æ¨ç†è€—æ—¶: {infer_time*1000:.1f}ms")
        
        # è§£æç»“æœ
        class_names = [
            "person", "bicycle", "car", "motorcycle", "airplane", "bus", "train", "truck", "boat",
            "traffic light", "fire hydrant", "stop sign", "parking meter", "bench", "bird", "cat",
            "dog", "horse", "sheep", "cow", "elephant", "bear", "zebra", "giraffe", "backpack",
            "umbrella", "handbag", "tie", "suitcase", "frisbee", "skis", "snowboard", "sports ball",
            "kite", "baseball bat", "baseball glove", "skateboard", "surfboard", "tennis racket",
            "bottle", "wine glass", "cup", "fork", "knife", "spoon", "bowl", "banana", "apple",
            "sandwich", "orange", "broccoli", "carrot", "hot dog", "pizza", "donut", "cake", "chair",
            "couch", "potted plant", "bed", "dining table", "toilet", "tv", "laptop", "mouse",
            "remote", "keyboard", "cell phone", "microwave", "oven", "toaster", "sink", "refrigerator",
            "book", "clock", "vase", "scissors", "teddy bear", "hair drier", "toothbrush"
        ]
        
        detections = []
        for pred in out:
            if len(pred) == 0:
                continue
            
            predn = np.copy(pred)
            scale_coords(img_tensor.shape[2:], predn[:, :4], (h_ori, w_ori))
            
            for det in predn:
                x1, y1, x2, y2, conf, cls_id = det
                cls_id = int(cls_id)
                
                detections.append({
                    "class_name": class_names[cls_id] if cls_id < len(class_names) else "unknown",
                    "confidence": float(conf),
                    "bbox": [int(x1), int(y1), int(x2), int(y2)]
                })
        
        logger.info(f"âœ… æ£€æµ‹åˆ° {len(detections)} ä¸ªç‰©ä½“: {[d['class_name'] for d in detections]}")
        
        # å¯è§†åŒ–
        for det in detections:
            x1, y1, x2, y2 = det["bbox"]
            cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 2)
            cv2.putText(img, f"{det['class_name']} {det['confidence']:.2f}",
                       (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)
        
        cv2.imshow("Detection", img)
        cv2.waitKey(3000)
        cv2.destroyAllWindows()
        
        return detections
        
    except Exception as e:
        logger.error(f"âŒ è§†è§‰æ£€æµ‹å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None


def test_target_matching(voice_target, detections):
    """æµ‹è¯•ç›®æ ‡åŒ¹é…"""
    logger.info("="*60)
    logger.info("æµ‹è¯•4: ç›®æ ‡åŒ¹é…")
    logger.info("="*60)
    
    # ä¸­è‹±æ–‡æ˜ å°„
    mapping = {
        "æ°´æ¯": "cup", "æ¯å­": "cup",
        "è‹¹æœ": "apple", "é¦™è•‰": "banana",
        "ç“¶å­": "bottle", "ç¢—": "bowl",
        "ä¹¦": "book", "æ‰‹æœº": "cell phone",
        "é¼ æ ‡": "mouse", "é”®ç›˜": "keyboard"
    }
    
    english_target = mapping.get(voice_target)
    if not english_target:
        logger.warning(f"âš ï¸ æœªæ‰¾åˆ°'{voice_target}'çš„æ˜ å°„")
        return None
    
    logger.info(f"ğŸ” æ˜ å°„: {voice_target} â†’ {english_target}")
    
    candidates = [d for d in detections if d["class_name"] == english_target]
    
    if not candidates:
        logger.warning(f"âŒ æœªæ£€æµ‹åˆ°: {english_target}")
        return None
    
    best = max(candidates, key=lambda x: x["confidence"])
    logger.info(f"âœ… åŒ¹é…æˆåŠŸ: {best['class_name']} (ç½®ä¿¡åº¦: {best['confidence']:.2%})")
    
    return best


def main():
    """ä¸»æµ‹è¯•æµç¨‹"""
    logger.info("\n" + "="*60)
    logger.info("ğŸ§ª è¯­éŸ³å¼•å¯¼æœºæ¢°è‡‚ç³»ç»Ÿ - é›†æˆæµ‹è¯•")
    logger.info("="*60 + "\n")
    
    # æµ‹è¯•1: è¯­éŸ³è¯†åˆ«
    voice_text = test_voice_recognition()
    if not voice_text:
        voice_text = "å¸®æˆ‘æ‹¿æ°´æ¯"  # ä½¿ç”¨é»˜è®¤æµ‹è¯•æ–‡æœ¬
        logger.info(f"ä½¿ç”¨é»˜è®¤æµ‹è¯•æ–‡æœ¬: {voice_text}")
    
    time.sleep(1)
    
    # æµ‹è¯•2: LLMè§£æ
    targets = test_llm_parsing(voice_text)
    if not targets:
        logger.error("âŒ æµ‹è¯•ä¸­æ–­: LLMè§£æå¤±è´¥")
        return
    
    target_name = targets[0]
    time.sleep(1)
    
    # æµ‹è¯•3: è§†è§‰æ£€æµ‹
    detections = test_vision_detection()
    if not detections:
        logger.error("âŒ æµ‹è¯•ä¸­æ–­: è§†è§‰æ£€æµ‹å¤±è´¥")
        return
    
    time.sleep(1)
    
    # æµ‹è¯•4: ç›®æ ‡åŒ¹é…
    matched = test_target_matching(target_name, detections)
    
    if matched:
        logger.info("\n" + "="*60)
        logger.info("âœ… å…¨æµç¨‹æµ‹è¯•æˆåŠŸ!")
        logger.info(f"   è¯­éŸ³è¾“å…¥: {voice_text}")
        logger.info(f"   ç›®æ ‡æå–: {target_name}")
        logger.info(f"   åŒ¹é…ç»“æœ: {matched['class_name']} (ç½®ä¿¡åº¦: {matched['confidence']:.2%})")
        logger.info(f"   è¾¹ç•Œæ¡†: {matched['bbox']}")
        logger.info("="*60)
    else:
        logger.warning("\nâš ï¸ æœªæ‰¾åˆ°åŒ¹é…ç›®æ ‡")


if __name__ == "__main__":
    main()
