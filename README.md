# ğŸ¤– Voice-Guided Robot Sorting System

<div align="center">

[![License](https://img.shields.io/badge/license-Apache%202.0-blue.svg)](LICENSE)
[![Python](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![ROS2](https://img.shields.io/badge/ROS2-Humble-blue.svg)](https://docs.ros.org/en/humble/)
[![YOLO](https://img.shields.io/badge/YOLO-v3--v11-yellow.svg)](https://github.com/ultralytics/ultralytics)

**An intelligent robotic sorting system integrating voice recognition, computer vision, and robotic manipulation**

[English](README.md) | [ä¸­æ–‡](README_CN.md)

</div>

---

### ğŸ“– Overview

This project implements an end-to-end robotic sorting system that combines:
- **Voice Recognition**: Natural language command processing
- **LLM Integration**: Intent extraction using large language models
- **Computer Vision**: YOLO-based object detection (v3-v11 supported)
- **Robot Control**: ROS2-powered robotic arm manipulation

**Supported Objects**: Apple ğŸ | Orange ğŸŠ | Cup â˜• | Bottle ğŸ¾

### âœ¨ Key Features

- ğŸ¯ **Voice-to-Action Pipeline**: Speak â†’ Detect â†’ Grasp â†’ Sort
- ğŸš€ **Hardware Acceleration**: NPU (Ascend 310B) / GPU (CUDA) support
- ğŸ”„ **Multi-Mode Operation**: Vision-only testing / Single execution / Continuous loop
- ğŸ“¦ **Modular Architecture**: Easy to extend and customize
- ğŸ› ï¸ **Production Ready**: Comprehensive logging and error handling

### ğŸ¬ Demo Workflow

```
User: "Please grab an apple"
  â†“
[Voice Recognition] â†’ Transcribe to text
  â†“
[LLM Parser] â†’ Extract intent: target="apple"
  â†“
[YOLO Detection] â†’ Locate apple in camera frame
  â†“
[Coordinate Mapping] â†’ Pixel â†’ Robot base coordinates
  â†“
[Inverse Kinematics] â†’ Calculate joint angles
  â†“
[Robot Execution] â†’ Grasp â†’ Move to sorting bin â†’ Release
```

### ğŸ—ï¸ System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    User Voice Command                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Voice Recognition Module (recognize_voice.py)               â”‚
â”‚  â”œâ”€ Audio Input (Microphone)                                 â”‚
â”‚  â””â”€ Speech-to-Text Conversion                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LLM Intent Parser (LLMæ„å›¾è¯†åˆ«.py)                           â”‚
â”‚  â”œâ”€ Natural Language Understanding                            â”‚
â”‚  â””â”€ Target Object Extraction                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Vision Perception Module (VisionPerception class)           â”‚
â”‚  â”œâ”€ YOLO Detection (v3-v11 supported)                        â”‚
â”‚  â”œâ”€ Backend: NPU/GPU/CPU auto-selection                      â”‚
â”‚  â””â”€ Object Localization                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Decision Layer (DecisionLayer class)                        â”‚
â”‚  â””â”€ Match voice target with visual detections                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Coordinate Mapping (CoordinateMapper class)                 â”‚
â”‚  â””â”€ Pixel coordinates â†’ Robot base frame                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Robot Control (RobotArmController class)                    â”‚
â”‚  â”œâ”€ ROS2 Service: Inverse Kinematics (trial_service)         â”‚
â”‚  â”œâ”€ Motion Planning: MoveIt2                                  â”‚
â”‚  â””â”€ Execution: Grasp â†’ Transport â†’ Release                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Directory Structure**:
```
â”œâ”€â”€ mindyolo-master/              # YOLO model training & inference
â”‚   â”œâ”€â”€ configs/                  # Model configs (YOLOv3-v11)
â”‚   â”œâ”€â”€ demo/                     # Demo scripts
â”‚   â”‚   â”œâ”€â”€ LLMæ„å›¾è¯†åˆ«.py        # LLM integration
â”‚   â”‚   â”œâ”€â”€ recognize_voice.py    # Voice recognition
â”‚   â”‚   â””â”€â”€ npupredict.py         # NPU-accelerated inference
â”‚   â””â”€â”€ deploy/                   # Model deployment tools
â”‚
â”œâ”€â”€ ros2_robot_arm/               # Robot control system
â”‚   â””â”€â”€ ros2_ws/                  # ROS2 workspace
â”‚       â””â”€â”€ src/
â”‚           â”œâ”€â”€ dofbot_moveit/    # Motion planning (IK server)
â”‚           â”œâ”€â”€ dofbot_info/      # Robot messages & services
â”‚           â””â”€â”€ dofbot_garbage_yolov5/  # Legacy detection module
â”‚
â”œâ”€â”€ voice_guided_robot_system.py  # Main program
â”œâ”€â”€ debug_check.py                # Diagnostic tool
â””â”€â”€ run_ubuntu.sh                 # Launch script
```

### ğŸ”§ Requirements

#### Hardware
- **OS**: Ubuntu 20.04 / 22.04 (recommended)
- **Robot**: Dofbot 5-DOF arm or compatible
- **Camera**: USB/CSI camera (640Ã—480 or higher)
- **Microphone**: For voice input
- **Optional**: Huawei Ascend NPU (310B) for acceleration

#### Software
| Component | Version | Required |
|-----------|---------|----------|
| Python | 3.8+ | âœ… |
| ROS2 | Humble | âœ… |
| PyTorch | 1.10+ | âœ… (CPU/GPU) |
| Ultralytics | Latest | âœ… |
| OpenCV | 4.x | âœ… |
| MindSpore | 2.x | âšª (for NPU) |
| CUDA | 11.x+ | âšª (for GPU) |

### ğŸš€ Quick Start

#### 1ï¸âƒ£ Install Dependencies

```bash
# Clone repository
git clone https://github.com/YOUR_USERNAME/voice-guided-robot-sorting.git
cd voice-guided-robot-sorting

# Install Python packages
pip3 install -r requirements.txt

# Install ROS2 Humble (if not installed)
# Ubuntu 22.04:
sudo apt update
sudo apt install ros-humble-desktop

# Install robot control library
cd ros2_robot_arm/0.py_install
pip3 install .
```

#### 2ï¸âƒ£ Configure API Keys

**Important**: Set up your API credentials before running the system.

```bash
# Copy the example environment file
cp .env.example .env

# Edit .env and fill in your API keys:
# - XFYUN_APPID: Get from https://console.xfyun.cn/
# - XFYUN_API_KEY: iFlytek ASR API key
# - XFYUN_API_SECRET: iFlytek ASR secret
# - DOUBAO_API_KEY: Get from https://console.volcengine.com/ark
```

**Load environment variables**:

```bash
# Linux/Mac
source .env
# Or
export $(cat .env | xargs)

# Windows PowerShell
Get-Content .env | ForEach-Object {
    $var = $_.Split('=')
    [Environment]::SetEnvironmentVariable($var[0], $var[1])
}
```

> **ğŸ”’ Security**: Never commit `.env` file to Git! It's already in `.gitignore`.

#### 3ï¸âƒ£ Build ROS2 Workspace

```bash
cd ros2_robot_arm/ros2_ws

# Source ROS2 environment
source /opt/ros/humble/setup.bash

# Build workspace
colcon build --symlink-install

# Source workspace
source install/setup.bash
```

> **âš ï¸ Important**: Always run `source /opt/ros/humble/setup.bash` before building!

#### 4ï¸âƒ£ Launch System

**Option A: Vision-Only Mode** (No robot required)
```bash
./run_ubuntu.sh
# Select [1] Vision Detection Only
```

**Option B: Full System** (Requires robot hardware)

*Terminal 1 - Start ROS2 service:*
```bash
source /opt/ros/humble/setup.bash
cd ros2_robot_arm/ros2_ws
source install/setup.bash
ros2 run dofbot_moveit dofbot_server
```

*Terminal 2 - Run main program:*
```bash
source /opt/ros/humble/setup.bash
cd ros2_robot_arm/ros2_ws
source install/setup.bash
cd ../../
python3 voice_guided_robot_system.py
```

**Option C: Use Launch Script**
```bash
chmod +x run_ubuntu.sh
./run_ubuntu.sh
# Select mode: [1] Vision | [2] Single Run | [3] Continuous
```

### ğŸ’¬ Usage Examples

#### Voice Commands (Natural Language)

```
User: "Please grab an apple"          â†’ System: Detects & picks apple
User: "å¸®æˆ‘æ‹¿ä¸€ä¸ªæ©˜å­"                  â†’ System: Detects & picks orange  
User: "I need a cup"                   â†’ System: Detects & picks cup
User: "æŠŠç“¶å­æ”¾åˆ°ç¯®å­é‡Œ"               â†’ System: Detects & picks bottle
```

**Supported languages**: English, Chinese (ä¸­æ–‡)

#### Supported Objects

| Object | English | Chinese | COCO Class |
|--------|---------|---------|------------|
| ğŸ | Apple | è‹¹æœ | `apple` |
| ğŸŠ | Orange | æ©˜å­/æ©™å­ | `orange` |
| â˜• | Cup | æ¯å­/æ°´æ¯ | `cup` |
| ğŸ¾ | Bottle | ç“¶å­ | `bottle` |

#### Sorting Bins Configuration

Each object is sorted to a predefined location:
```python
SORT_POSITIONS = {
    "apple":  Position 1 (Front-Left)   # [45Â°, 50Â°, 20Â°, 60Â°, 265Â°]
    "orange": Position 2 (Back-Left)    # [27Â°, 75Â°, 0Â°, 50Â°, 265Â°]
    "cup":    Position 3 (Back-Right)   # [147Â°, 75Â°, 0Â°, 50Â°, 265Â°]
    "bottle": Position 4 (Front-Right)  # [133Â°, 50Â°, 20Â°, 60Â°, 265Â°]
}
```

### âš™ï¸ Configuration

#### System Config (`system_config.py`)

```python
# Camera settings
CAMERA_ID = 0              # USB camera index
RESOLUTION = (640, 480)    # Image size

# Detection parameters
CONF_THRESHOLD = 0.5       # Confidence threshold
IOU_THRESHOLD = 0.65       # NMS IoU threshold

# Robot settings
ROBOT_SPEED = 1000         # Servo speed (ms)
GRASP_ANGLE = 130          # Gripper close angle

# Acceleration
DEVICE = "auto"            # auto | npu | cuda | cpu
```

#### YOLO Model Selection

Supported models in `mindyolo-master/configs/`:
- **YOLOv3** - Classic baseline
- **YOLOv5** - Fast & accurate (s/m/l/x variants)
- **YOLOv7** - High performance
- **YOLOv8** - Latest Ultralytics (recommended)
- **YOLOv9/v10/v11** - Cutting edge
- **YOLOX** - Anchor-free alternative

To switch models, edit `voice_guided_robot_system.py`:
```python
config = {
    "model_path_pt": "path/to/yolov8s.pt",  # Change model here
    # ...
}
```

### ğŸ” Troubleshooting

<details>
<summary><b>âŒ ROS2 Service Timeout</b></summary>

**Error**: `âŒ [å¤±è´¥] é€†è¿åŠ¨å­¦æœåŠ¡è¶…æ—¶(5ç§’å†…æœªå“åº”)`

**Solution**: Start the inverse kinematics service:
```bash
source /opt/ros/humble/setup.bash
cd ros2_robot_arm/ros2_ws && source install/setup.bash
ros2 run dofbot_moveit dofbot_server
```

Verify service is running:
```bash
ros2 service list | grep trial_service
```
</details>

<details>
<summary><b>âŒ Module Import Error</b></summary>

**Error**: `ModuleNotFoundError: No module named 'mindyolo'`

**Solution**: Install mindyolo package:
```bash
cd mindyolo-master
pip3 install -e .
```
</details>

<details>
<summary><b>âŒ ROS2 Build Failed</b></summary>

**Error**: `Could not find a package configuration file provided by "ament_cmake"`

**Solution**: Source ROS2 environment before building:
```bash
source /opt/ros/humble/setup.bash
cd ros2_robot_arm/ros2_ws
colcon build --symlink-install
```
</details>

<details>
<summary><b>âŒ Camera Not Found</b></summary>

**Error**: `âŒ æ— æ³•æ‰“å¼€æ‘„åƒå¤´`

**Solution**: 
1. Check camera connection: `ls /dev/video*`
2. Test camera: `cheese` or `v4l2-ctl --list-devices`
3. Change camera ID in config: `CAMERA_ID = 1` (try 0, 1, 2...)
</details>

<details>
<summary><b>âŒ Voice Recognition Not Working</b></summary>

**Solution**:
- Check microphone permissions
- Test microphone: `arecord -d 5 test.wav && aplay test.wav`
- Install audio libraries: `sudo apt install portaudio19-dev`
</details>

<details>
<summary><b>ğŸ› ï¸ Diagnostic Tool</b></summary>

Run automated system check:
```bash
python3 debug_check.py
```

This checks:
- âœ… ROS2 environment
- âœ… Service availability  
- âœ… Serial port connection
- âœ… Python dependencies
</details>

### ğŸ§ª Development

#### Testing

```bash
# Test vision detection only
python3 test_4class_detection.py

# Test full system integration
python3 test_integration.py

# Run diagnostic check
python3 debug_check.py
```

#### Model Training

To train custom YOLO models:
```bash
cd mindyolo-master

# Prepare dataset (COCO format)
# Edit config: configs/yolov8/yolov8s.yaml

# Train model
python train.py --config configs/yolov8/yolov8s.yaml \
                --data configs/coco.yaml \
                --epochs 100

# Export for deployment
python export.py --config configs/yolov8/yolov8s.yaml \
                 --weight runs/train/weights/best.ckpt
```

See `mindyolo-master/GETTING_STARTED.md` for details.

#### Adding New Object Classes

1. **Update mapping** in `voice_guided_robot_system.py`:
```python
OBJECT_MAPPING = {
    "è‹¹æœ": "apple",
    "your_object": "coco_class_name",  # Add here
}
```

2. **Add sorting position**:
```python
SORTING_POSITIONS = {
    "coco_class_name": [j1, j2, j3, j4, j5],  # Joint angles
}
```

3. **Update LLM prompts** in `demo/LLMæ„å›¾è¯†åˆ«.py`

### ğŸ› ï¸ Tech Stack

| Layer | Technology |
|-------|------------|
| **Vision** | YOLO v3-v11, YOLOX (MindSpore/PyTorch/Ultralytics) |
| **Voice** | Speech Recognition Library |
| **NLP** | Large Language Model (LLM) Integration |
| **Robot Control** | ROS2 Humble + MoveIt2 |
| **Kinematics** | Orocos KDL |
| **Acceleration** | Huawei Ascend NPU / NVIDIA CUDA |
| **Framework** | Python 3.8+, C++ 14 |

### ğŸ“š Documentation

- [`ARCHITECTURE.txt`](ARCHITECTURE.txt) - System architecture details
- [`SYSTEM_USAGE.txt`](SYSTEM_USAGE.txt) - Usage guide
- [`UBUNTU_SETUP.txt`](UBUNTU_SETUP.txt) - Ubuntu setup instructions
- [`å®Œæ•´éƒ¨ç½²æŒ‡å—.txt`](å®Œæ•´éƒ¨ç½²æŒ‡å—.txt) - Complete deployment guide (ä¸­æ–‡)
- [`4ç±»ç‰©å“åˆ†æ‹£ç³»ç»Ÿæ›´æ–°è¯´æ˜.md`](4ç±»ç‰©å“åˆ†æ‹£ç³»ç»Ÿæ›´æ–°è¯´æ˜.md) - Update log (ä¸­æ–‡)

### ğŸ¤ Contributing

Contributions are welcome! Please:
1. Fork the repository
2. Create your feature branch: `git checkout -b feature/AmazingFeature`
3. Commit changes: `git commit -m 'Add AmazingFeature'`
4. Push to branch: `git push origin feature/AmazingFeature`
5. Open a Pull Request

### ğŸ“„ License

This project is licensed under the Apache 2.0 License - see [LICENSE](LICENSE) file for details.

### ğŸ™ Acknowledgments

- [Ultralytics YOLO](https://github.com/ultralytics/ultralytics) - Object detection models
- [MindSpore](https://www.mindspore.cn/) - Deep learning framework
- [ROS2](https://docs.ros.org/) - Robot Operating System
- [Orocos KDL](https://www.orocos.org/kdl.html) - Kinematics library

### ğŸ“§ Contact

- **Issues**: [GitHub Issues](https://github.com/YOUR_USERNAME/voice-guided-robot-sorting/issues)
- **Discussions**: [GitHub Discussions](https://github.com/YOUR_USERNAME/voice-guided-robot-sorting/discussions)

---

<div align="center">

**â­ If you find this project helpful, please give it a star! â­**

**ğŸ’¡ First-time users: Start with Vision-Only mode to verify detection before enabling full system**

Made with â¤ï¸ by the Robot Vision Team

</div>
